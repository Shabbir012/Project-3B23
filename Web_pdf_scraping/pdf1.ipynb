{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**As part of our sources for multi modal sentiment analysis, we decided to scrape pdfs to increase our dataset. There are various libraries that could be used. In this instance, pymupdf library is used.**"
      ],
      "metadata": {
        "id": "IAhKrYS9SuxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install libraries\n",
        "!pip install pymupdf\n"
      ],
      "metadata": {
        "id": "tfSzcOWF7D7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the libraries\n",
        "import fitz\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "DRtNTZjJ7HOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input the path of the pdf\n",
        "File_Path= \"/content/Artificial-Intelligence-for-human-centric-society (1) large resource 2024 (1).pdf\"\n"
      ],
      "metadata": {
        "id": "hSXQChaK7JaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entire book as text starting from 0 to last page\n",
        "# get all the text as list\n",
        "from sys import ps1\n",
        "def get_book_content(pdf_file):\n",
        "  num_page = pdf_file.page_count\n",
        "  book_content = []\n",
        "  for i in range(7,29):\n",
        "    page = pdf_file.load_page(i)\n",
        "    page_text =  page.get_text().replace(\"\\n\", \" \")\n",
        "    raw_text = page_text.replace(\"\\t\", \" \")\n",
        "\n",
        "    book_content.append(raw_text)\n",
        "\n",
        "  return book_content\n"
      ],
      "metadata": {
        "id": "G9U2Obig7hGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call get_book_content function\n",
        "book = get_book_content(pdf)\n",
        "book"
      ],
      "metadata": {
        "id": "Vhll3q7M7jh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize into sentences\n",
        "sentencelist = []\n",
        "for i in range(0, 22):\n",
        "# Create a sentence tokenizer\n",
        "  tokenizer = nltk.sent_tokenize(book[i])\n",
        "\n",
        "# Print the extracted sentences\n",
        "  for sentence in tokenizer:\n",
        "    sentencelist.append(sentence)\n",
        "\n",
        "sentencelist"
      ],
      "metadata": {
        "id": "DllJTwA0XK5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert list to data frame\n",
        "df = pd.DataFrame(sentencelist, columns = ['Text'])\n",
        "df"
      ],
      "metadata": {
        "id": "2oYwN8JDLxbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save to a csv file\n",
        "df.to_csv(\"Dataset14_pdf\")"
      ],
      "metadata": {
        "id": "m5oL87P1MWnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RvdAhK5WK8ti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}